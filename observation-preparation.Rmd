---
title: "observation-preparation"
author: "jafshin"
date: "14/05/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(fs)
library(sf)
library(lubridate)
library(readxl)
# library(rosm)
library(ggspatial)
library(lwgeom)
library(stringr)
library(igraph)
```

## Car traffic data

### Car traffic data cleaning 

Reading input 

```{r}
###############################################################################
# Day of Week where 1=Monday, 2=Tuesday,3=Wednesday,                          #
# 4=Thursday,5=Friday,6=Saturday and 7=Sunday                                 #
###############################################################################
# A=Actual count, Ehis=Estimate based on historic data,                       #
# Egmr=Estimate based on global substitution, Erte=Estimate based on route    #
###############################################################################

col_spec <- cols(
  .default = col_double(),
  HMGNS_LNK_DESC = col_character(),
  FLOW = col_character(),
  COUNT_TYPE = col_character(),
  PERIOD_TYPE = col_character()
)

# carData <- read_csv("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/car/TYPICAL_HOURLY_VOLUME_DATA.csv", col_types = col_spec)
carData <- read_csv("./data/observations/car/TYPICAL_HOURLY_VOLUME_DATA.csv", col_types = col_spec)

carDataFiltered <- carData %>% 
  filter(PERIOD_TYPE=="SCHOOL TERM/NORMAL") %>% 
  filter(DOW==3) %>% 
  # Filter to those with high enough number traffic
  mutate(HMGNS_FLOW_ID=as.character(HMGNS_FLOW_ID)) %>% 
  mutate(HMGNS_LNK_ID=as.character(HMGNS_LNK_ID)) %>% 
  dplyr::select(-DOW) %>% 
  mutate(totalVol=rowSums(across(where(is.numeric)))) 

# carDataFilter %>% distinct(HMGNS_FLOW_ID) %>% arrange(HMGNS_FLOW_ID)
```

Reading Greater Melbourne Boundaries for mapping:

```{r}
# GMelbBoundary <- read_sf("~/ownCloud/io.github.jafshin/calibration-validation/data/boundaries/ABS_Boundaries/1270055001_sa1_2016_aust_shape/SA1_2016_AUST.shp") %>% 
GMelbBoundary <- read_sf("./data/boundaries/1270055001_sa1_2016_aust_shape/SA1_2016_AUST.shp") %>% 
  st_transform(28355) %>% 
  filter(GCC_NAME16=="Greater Melbourne") %>% 
  summarise(geometry=st_union(geometry)) 
```

Adding geometry to the roads

```{r}
# TIS_ROUTE_FLOW=	Traffic Information Route Flow (1=Generally out of Melb, Generally 0= into Melb)
# 

# HMGNS_FLOW_ID	Text	10	Homogeneous flow id eg 1234. An internal reference for Homogenous Flow. Homogeneous flow - The traffic volume information associated with the traffic flow along a link that is representative of all travel along the whole link

# ROAD_NBR	Number	8	ROAD_NUMBER

# htvnData <- st_read("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/car/homogenous_traffic_flow-shp/3ad2d9d5-dd49-40bf-86c4-ccf114d2e4582020328-1-cs85vw.3kgy7.shp") %>% 
htvnData <- st_read("./data/observations/car/homogenous_traffic_flow-shp/3ad2d9d5-dd49-40bf-86c4-ccf114d2e4582020328-1-cs85vw.3kgy7.shp") %>% 
  st_transform(28355) %>% 
  mutate(HMGNS_FLOW_ID=as.character(HMGNS_FLOW))

# glimpse(htvnData)

# htvnData %>% distinct(HMGNS_FLOW) %>% arrange(HMGNS_FLOW)

carDataJoined <- carDataFiltered %>%
  left_join(htvnData, by="HMGNS_FLOW_ID") %>% 
  st_as_sf()

st_write(carDataJoined, "carDataJoined.sqlite", delete_dsn = T)
```

Crop to greater Melbourne

```{r}
# gMelbBoundary <- st_read("~/ownCloud/Data/ABS_Boundaries/GreaterMelbourneArea/GMEL_SA4_2016_Polygons.sqlite")  %>% 
gMelbBoundary <- st_read("./data/boundaries/GreaterMelbourneArea/GMEL_SA4_2016_Polygons.sqlite")  %>% 
  st_transform(28355) %>%
  summarise()  # to dissolve SA4s (without this, roads get split at SA4 boundaries)
  
carDataCroped <- carDataJoined %>% 
  st_intersection(gMelbBoundary)

st_write(carDataCroped, "carDataCropedSmall.sqlite", delete_dsn = T)
```

### AM Peak data

```{r}

amCarData <- carDataCroped %>% 
  filter(TIS_ROUTE1==0)

```

finding high volume AM roads

```{r}

amHighVolRoads <- amCarData %>% 
  st_drop_geometry() %>% 
  group_by(ROAD_NBR) %>% 
  summarise(roadVol=mean(totalVol)) %>% 
  slice_max(order_by = roadVol, prop=0.1)

amHighVolCarData <- amCarData %>% 
  filter(ROAD_NBR %in% amHighVolRoads$ROAD_NBR) %>% 
  group_by(ROAD_NBR) %>% 
  slice_max(order_by = totalVol, n=1)

st_write(amHighVolCarData, "amHighVolCarData.sqlite", delete_dsn = T)
```

Plot of AM Peak Count locations

```{r}
amHighVolCarData %>% 
  ggplot() +
  annotation_map_tile(type="osmgrayscale",zoom=9, alpha=0.6) +
  geom_sf(aes(fill=totalVol)) +
  scale_fill_viridis_c(trans = "sqrt", alpha = .8) 

```

### PM Peak data

```{r}

pmCarData <- carDataCroped %>% 
  filter(TIS_ROUTE1==1)

```

finding high volume PM roads

```{r}

pmHighVolRoads <- pmCarData %>% 
  st_drop_geometry() %>% 
  group_by(ROAD_NBR) %>% 
  summarise(roadVol=mean(totalVol)) %>% 
  slice_max(order_by = roadVol, prop=0.1)

pmHighVolCarData <- pmCarData %>% 
  filter(ROAD_NBR %in% pmHighVolRoads$ROAD_NBR) %>% 
  group_by(ROAD_NBR) %>% 
  slice_max(order_by = totalVol, n=1)

st_write(pmHighVolCarData, "pmHighVolCarData.sqlite", delete_dsn = T)
```


Plot of AM Peak Count locations

```{r}
pmHighVolCarData %>% 
  ggplot() +
  annotation_map_tile(type="osmgrayscale",zoom=9, alpha=0.6) +
  geom_sf(aes(fill=totalVol)) +
  scale_fill_viridis_c(trans = "sqrt", alpha = .8) 

```

## Bicycle traffic simulation

Reading the input files

```{r}

# paths <- dir_ls("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/bicycle/2018-03/",glob = "*.zip" )
paths <- dir_ls("./data/observations/bicycle/2018-03/",glob = "*.zip" )


# Extracting all the bike count data
walk(paths, ~unzip(.x, exdir = "data/cyclingVolFiles"))

# reading the data it
cyclingVol_paths <- dir_ls("./data/cyclingVolFiles/")
col_spec <- cols(
  DATA_TYPE = col_character(),
  TIS_DATA_REQUEST = col_double(),
  SITE_XN_ROUTE = col_double(),
  LOC_LEG = col_double(),
  DATE = col_character(),
  TIME = col_time(format = ""),
  CLASS = col_double(),
  LANE = col_double(),
  SPEED = col_double(),
  WHEELBASE = col_double(),
  HEADWAY = col_double(),
  GAP = col_double(),
  AXLE = col_double(),
  AXLE_GROUPING = col_double(),
  RHO = col_double(),
  VEHICLE = col_character(),
  DIRECTION = col_character()
)

cyclingVol <- map_dfr(cyclingVol_paths, ~ read_csv(.x, col_types = col_spec))
```

A quick look at what we've got

```{r}
glimpse(cyclingVol)
```

Filtering to mid-week day data

```{r}
cyclingVolFiltered <- cyclingVol %>% 
  filter(!DATE%in%c("12/03/2018", "30/03/2018", "31/03/2018")) %>% # Removing public holidays
  mutate(DATE=dmy(DATE)) %>% 
  mutate(DOW = wday(DATE, label=TRUE)) %>% 
  filter(DOW%in%c("Tue", "Wed", "Thu")) # Selecting mid-week days
  
```


Counting cycling trips based on counter location, direcation/leg, date and hour of the day

```{r}
cyclingVolCounted <- cyclingVolFiltered %>% 
  mutate(hour=hour(TIME)) %>% 
  group_by(SITE_XN_ROUTE, LOC_LEG, DIRECTION, DATE, hour) %>% 
  summarise(count=n()) %>% 
  ungroup()

glimpse(cyclingVolCounted)

```

Aggregating count data by averaging over all days

```{r}
cyclingVolAverage <- cyclingVolCounted %>% 
  group_by(SITE_XN_ROUTE, LOC_LEG, DIRECTION, hour) %>% 
  summarise(avgCount=round(mean(count))) %>% 
  ungroup() %>% 
  mutate(siteNumber=as.character(SITE_XN_ROUTE)) %>% 
  mutate(legNumber=as.character(LOC_LEG)) %>% 
  dplyr::select(siteNumber, legNumber, dir=DIRECTION, hour, count=avgCount) 

glimpse(cyclingVolAverage)

```

### Having a look at the final data 

```{r}

cyclingVolAverage %>% count(siteNumber, legNumber, dir) %>% 
  ggplot(aes(x=dir, y=n))+
  geom_col(aes(fill=dir))

```

Plotting one of the count locations

```{r}

cyclingVolAverage %>% filter(siteNumber==6411) %>% 
  ggplot(aes(x=hour, y=count)) +
  geom_point(aes(color=dir)) +
  geom_line(aes(color=dir))
  
```

Plotting aggregated values 

```{r}

cyclingVolAverage %>% 
  group_by(hour, siteNumber) %>% 
  summarise(routeVol=sum(count)) %>%
  summarise(totalVol=mean(routeVol)) %>% 
  ggplot(aes(x=hour, y=totalVol)) +
  geom_point() +
  geom_line()

```

### Adding coordinates to the count points

Reading the location points

```{r}

# cycleCountersMeta <- read_xlsx("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/bicycle/VicRoads_Bike_Site_Number_Listing.xlsx",
cycleCountersMeta <- read_xlsx("./data/observations/bicycle/VicRoads_Bike_Site_Number_Listing.xlsx",
          skip=2, col_names=c("id", "site", "gps", "desc", "comment")) %>% 
  dplyr::filter(!is.na(id))

cycleCountersMeta %>% glimpse()

```

```{r}
cycleCounterCoordinated <- cycleCountersMeta %>% 
  # Extracting the site number
  mutate(siteNumber=stringr::str_extract(site,pattern = "X[0-9]+")) %>% 
  mutate(siteNumber=stringr::str_extract(siteNumber,pattern = "[0-9]+")) %>%
  # Extracting the leg number
  mutate(legNumber=stringr::str_extract(site,pattern = "P[0-9]+")) %>% 
  mutate(legNumber=stringr::str_extract(legNumber,pattern = "[0-9]+")) %>% 
  # Extracting the long lat
  mutate(lat=stringr::str_extract(gps,pattern = "-[0-9]+.[0-9]+")) %>% 
  mutate(long=stringr::str_extract(gps,pattern = "\\+[0-9]+.[0-9]+")) %>% 
  st_as_sf(coords = c("long", "lat"), crs = 4326) %>% 
  st_transform(28355) %>% 
  dplyr::select(siteNumber, legNumber, desc, comment, geometry)

cycleCounterCoordinated %>% glimpse()
```

Joining cooridates to the count data

```{r}

cyclingVolCoordinated <- cyclingVolAverage %>% 
  left_join(cycleCounterCoordinated, by = c("siteNumber","legNumber")) %>% 
  st_as_sf() %>% 
  filter(!is.na(desc))

st_write(cyclingVolCoordinated, "cyclingVolCoordinated.sqlite", delete_layer=T)

cyclingVolCoordinated %>% glimpse()
```

Mapping selected cycling count locations

```{r}
cyclingVolCoordinated %>% 
  group_by(siteNumber,legNumber) %>% 
  summarise(totalVol=sum(count)) %>%  
  ggplot() +
  annotation_map_tile(type="osmgrayscale",zoom=10, alpha=0.8) +
  geom_sf(aes(color=totalVol))  
```

## Walking Data

Reading the walk count data
Source data: 
https://data.melbourne.vic.gov.au/Transport/Pedestrian-Counting-System-Monthly-counts-per-hour/b2ak-trbp

```{r}
col_spec <- cols(
  ID = col_double(),
  Date_Time = col_character(),
  Year = col_double(),
  Month = col_character(),
  Mdate = col_double(),
  Day = col_character(),
  Time = col_double(),
  Sensor_ID = col_double(),
  Sensor_Name = col_character(),
  Hourly_Counts = col_double()
)


# walkData <- read_csv("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/walk/Pedestrian_Counting_System_-_Monthly__counts_per_hour_.csv", col_types = col_spec) 
walkData <- read_csv("./data/observations/walk/Pedestrian_Counting_System_-_Monthly__counts_per_hour_.csv", col_types = col_spec) 

walkData %>% glimpse()
```

```{r}
walkDataFiltered <- walkData %>% 
  filter(Year==2018,Month=="March") %>% 
  filter(Day%in%c("Tuesday", "Wednesday", "Thursday"))

walkDataFiltered %>% count(Year,Month,Day)
```

```{r}
walkDataFiltered %>% glimpse()
```


Adding location to the count data


Source for sensor locations:
https://data.melbourne.vic.gov.au/Transport/Pedestrian-Counting-System-Sensor-Locations/h57g-5234

```{r}

col_spec <- cols(
  sensor_id = col_double(),
  sensor_description = col_character(),
  sensor_name = col_character(),
  installation_date = col_date(format = ""),
  status = col_character(),
  note = col_character(),
  direction_1 = col_character(),
  direction_2 = col_character(),
  latitude = col_double(),
  longitude = col_double(),
  location = col_character()
)

# walkSensorLocs <- read_csv("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/walk/Pedestrian_Counting_System_-_Sensor_Locations.csv", col_types = col_spec)
walkSensorLocs <- read_csv("./data/observations/walk/Pedestrian_Counting_System_-_Sensor_Locations.csv", col_types = col_spec)

walkSensorLocs %>% glimpse()
```

```{r}
walkDataJoined <- walkDataFiltered %>% 
  left_join(walkSensorLocs, by=c("Sensor_ID"="sensor_id")) 

walkDataJoined %>% glimpse()
```


```{r}
walkDataCoordinated <- walkDataJoined %>% 
  st_as_sf(coords = c("longitude", "latitude"), crs = 4326) %>% 
  st_transform(28355) %>% 
  dplyr::select(ID,Sensor_ID, Hourly_Counts, direction_1, direction_2,Time, geometry)

walkDataCoordinated %>% glimpse()
```

Mapping walking count point locations
```{r}
walkDataCoordinated %>% 
  group_by(Sensor_ID) %>% 
  summarise(totalVol=sum(Hourly_Counts)) %>% 
  ggplot() +
  annotation_map_tile(type="osmgrayscale",zoom=12, alpha=0.8) +
  geom_sf(aes(color=totalVol))  
```

## Public transport

### Station patronage

Reading stations' patronage survey data

```{r}

# stationData <- readxl::read_excel("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/pt/2016 Station Access Mode.xlsx",sheet = "Sheet2", skip = 3)  
stationData <- readxl::read_excel("./data/observations/pt/2016 Station Access Mode.xlsx",sheet = "Sheet2", skip = 3)  

  
stationDataFiltered <- stationData %>%   
  dplyr::select(-`Station Group`) %>% 
  filter(!is.na(Station)) %>% 
  mutate(stationID=stringr::str_extract(Station,pattern = "[0-9]+"))  %>% 
  mutate(stationName=stringr::str_extract(Station,pattern = "[A-z](.*)"))  
```

Reading stop locations from GTFS data

```{r}
col_spec <- cols(
  stop_id = col_double(),
  stop_name = col_character(),
  stop_lat = col_double(),
  stop_lon = col_double()
)

# stationLocations <- read_csv("~/ownCloud/io.github.jafshin/calibration-validation/data/observations/pt/stops.txt", col_types = col_spec) %>% 
stationLocations <- read_csv("./data/observations/pt/stops.txt", col_types = col_spec) %>% 
  mutate(stationID=as.character(stop_id)) %>% 
  # filter to rows containing 'Railway Station' and not '/' (used for bus or tram stops at stations) 
  filter(grepl("Railway Station", stop_name) & !grepl("/", stop_name)) %>%
  # replace the pattern 'space + Railway + any number of other characters' with nothing
  mutate(stationName = gsub(" Railway.*","", stop_name)) %>%
  # fix some name mismatches between patronage and GTFS names
  mutate(stationName = if_else(stationName=="McKinnon", "Mckinnon",
                               if_else(stationName=="Jolimont-MCG", "Jolimont",
                                       if_else(stationName=="Showgrounds", "Showgrounds Station",
                                               stationName)))) %>%
  dplyr::select(stationName, stop_lat, stop_lon)

```

Joining stop locations to the patronage data

```{r}
stationDataCoordinated <- stationDataFiltered %>% 
  left_join(stationLocations, by="stationName") %>%
  # remove duplicates (eg where Vline contains metro and Vline with same name)
  distinct(stationName, .keep_all=T)  

write_csv(stationDataCoordinated, "stationDataCoordinated.csv")
stationDataCoordinated %>% glimpse()

```

Plotting the Station data

```{r}
stationDataWithGeom <- stationDataCoordinated %>% 
  filter(!is.na(stop_lat)) %>% 
  st_as_sf(coords = c("stop_lon", "stop_lat"), remove=F, crs = 4326) %>% 
  st_transform(28355) %>% 
  dplyr::select(stationName, stationID, Total=`Total Result`, geometry)


```


Map of the train stops included for the calibration: 

```{r}
stationDataWithGeom %>% 
  st_intersection(GMelbBoundary) %>% 
  ggplot() +
  annotation_map_tile(type="osmgrayscale",zoom=9) +
  geom_sf(aes(color=Total, size= Total)) +
  scale_fill_viridis_c(trans = "sqrt", alpha = .4) 

```

```{r}
st_write(stationDataWithGeom, "stationDataWithGeom.sqlite", delete_dsn = T)
```


### Adding links from MATSim Network

Make a subgraph of links and nodes that includes stations, and filter to subgraph

```{r}
# read in links and nodes; filter to PT over 400m (smallest station distance Riversdale-Willison 418m:
# https://maps.philipmallis.com/distances-between-melbourne-railway-stations-a-quick-map/ )
links <- st_read("./generatedNetworks/MATSimMelbNetwork.sqlite", layer = "links") %>%
  mutate(link_id = row_number()) %>%
  filter(highway == "pt" & length > 400)

nodes <- st_read("./generatedNetworks/MATSimMelbNetwork.sqlite", layer = "nodes") %>%
   filter(id %in% links$from_id | id %in% links$to_id)

# make graph; find nodes from largest connected sub-graph; filter nodes and links accordingly
# see https://stackoverflow.com/questions/64344845/getting-the-biggest-connected-component-in-r-igraph
graph <- graph_from_data_frame(links, directed = F, vertices = nodes)
   
components <- clusters(graph)
biggest_cluster_id <- which.max(components$csize)
vertices <- V(graph)[components$membership == biggest_cluster_id]
vert_ids <- as.numeric(vertices$name)

nodes <- nodes %>%
  filter(id %in% vert_ids) %>%
  st_intersection(gMelbBoundary)

links <- links %>%
  filter(from_id %in% nodes$id & to_id %in% nodes$id)

```


Map of the remaining network to check that it appears to include stations

```{r}
ggplot() + 
  geom_sf(data = links, color = 'red') +
  geom_sf(data = nodes, color = 'blue')

```


Find the nearest node to each station, and add to patronage data

```{r}
# read in station patronage data, and add column for node_id
stationDataWithGeom <- st_read("./stationDataWithGeom.sqlite", layer = "stationdatawithgeom")

# find nearest nodes (st_nearest_feature returns the index; find the corresponding node id in 'nodes')
nearest.nodes <- nodes[st_nearest_feature(stationDataWithGeom, nodes), "id"] %>%
  st_drop_geometry()

# add to station patronage data - but not for Flemington Racecourse and Showgrounds: they are not
# in the links/nodes network, so their 'nearest nodes' are distant station
stationDataWithGeom <- stationDataWithGeom %>%
  mutate(node_id = ifelse(stationname %in% c("Showgrounds Station", "Flemington Racecourse"), NA,
                          nearest.nodes$id))

# write output
st_write(stationDataWithGeom, "stationDataWithGeom.sqlite", delete_dsn = T)

```

Checking the output

```{r}
# load patronage data (stops) and connecting links
rail.stops <- st_read("./stationDataWithGeom.sqlite", layer = "stationdatawithgeom")
rail.lines <- links %>%
  filter(from_id %in% rail.stops$node_id & to_id %in% rail.stops$node_id)

# check that it appears to be a connected network (except for Showgrounds and Flemington Racecourse)
# note that there are also  other links for express services
# note link gap between Highett and Cheltenham, because Southland station missing from patronage data
ggplot() + 
  geom_sf(data = rail.stops, color = 'red') +
  geom_sf(data = rail.lines, color = 'blue')
```

```{r}
# make graph to check that all stops are in fact connected
rail.graph <- graph_from_data_frame(rail.lines, directed = F)

# count vertices in largest subgraph
components <- clusters(rail.graph)
biggest_cluster_id <- which.max(components$csize)
vertices <- V(rail.graph)[components$membership == biggest_cluster_id]
no.vertices <- length(vertices)
no.vertices  # should be 216 (218 stations in patronage data, excl. Showgrounds and Flemington Racecourse)

```



#### Other potential sources for pt data: 
 - Train: https://philipmallis.com/blog/2019/02/14/station-patronage-in-victoria-2013-2018/
- Bus: https://discover.data.vic.gov.au/dataset/ptv-metro-tram-stops